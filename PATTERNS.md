# Паттерны: откуда они взялись

## Как появился подход

На LeetCode больше 3000 задач. Люди годами решали их вразнобой и заметили: задачи повторяются. Не буквально — но приёмы одни и те же. В 2019 году **Arslan Ahmad** (бывший инженер Facebook и Microsoft) оформил это наблюдение в курс [Grokking the Coding Interview](https://www.designgurus.io/course/grokking-the-coding-interview). Он выделил ~15 паттернов, которые покрывают большинство задач. Параллельно появился список [Blind 75](https://www.teamblind.com/post/New-Year-Gift---Curated-List-of-Top-75-LeetCode-Questions-to-Save-Your-Time-OaM1orEU) — 75 ключевых задач, тоже сгруппированных по приёмам.

Но сами паттерны придумал не Arslan. Они старше LeetCode, старше собеседований и старше большинства языков программирования.

## Откуда взялся каждый паттерн

### Two Pointers

Приём появился вместе с первыми алгоритмами сортировки. **Tony Hoare** в 1961 году придумал QuickSort — и там два указателя идут навстречу друг другу, разделяя массив на части. Это один из первых случаев, когда два указателя использовались осознанно как приём. С тех пор идея «пройти массив с двух сторон» стала базовой.

### Sliding Window

Корни уходят в обработку сигналов и сетевые протоколы. В TCP (1974, Vint Cerf и Bob Kahn) «скользящее окно» управляет потоком данных — отправитель не шлёт новые пакеты, пока окно не сдвинется. В алгоритмах идея та же: фиксированный или гибкий «кусок» данных, который едет по массиву.

### Binary Search

Один из древнейших алгоритмов. Идея описана ещё в **1946 году** (John Mauchly), но первая корректная реализация без багов появилась только в **1962** (спустя 16 лет!). Забавный факт: баг с переполнением в бинарном поиске нашли в Java-библиотеке в 2006 году — через 60 лет после изобретения алгоритма.

### HashMap / HashSet

Хеширование придумал **Hans Peter Luhn** в IBM в **1953 году** — для быстрого поиска документов. Он запатентовал идею «хеш-таблицы» до того, как большинство компьютеров вообще существовало. Коллизии (когда два ключа попадают в одну ячейку) решают цепочками — это тоже его идея.

### Stack

**Alan Turing** описал стековую структуру в 1946 году. Но в привычном виде стек появился в **1957** — Friedrich Bauer и Klaus Samelson предложили его для вычисления математических выражений. Каждый раз, когда ты нажимаешь Ctrl+Z — работает стек. Скобки в коде проверяются стеком. Кнопка «назад» в браузере — тоже стек.

### Trees / BFS / DFS

Деревья как структуру данных формализовал **Arthur Cayley** ещё в **1857 году** — в математике, задолго до компьютеров. BFS (обход в ширину) описал **Edward Moore** в 1959 для поиска кратчайшего пути в лабиринте. DFS (обход в глубину) формализовал **Charles Pierre Trémaux** — французский математик, который в **XIX веке** придумал алгоритм выхода из лабиринта: иди вглубь, упёрся в тупик — возвращайся и попробуй другой путь.

### Dynamic Programming

Название придумал **Richard Bellman** в **1950-х**. Он работал в RAND Corporation на военные проекты и выбрал слово «programming» не в смысле кода, а в смысле «планирование». А «dynamic» добавил, потому что звучало внушительно и чиновники не задавали лишних вопросов о финансировании. Сам Bellman так и говорил — название выбрано, чтобы не злить конгрессменов. Суть: если задача распадается на подзадачи, которые повторяются — решай каждую один раз и запоминай ответ.

### Greedy

Жадные алгоритмы использовались задолго до формального названия. **Kruskal** (1956) и **Prim** (1957) построили алгоритмы минимального остовного дерева — на каждом шаге бери самое дешёвое ребро. **Dijkstra** (1959) нашёл кратчайший путь тем же принципом — всегда иди к ближайшей непосещённой вершине. Термин «greedy» закрепился позже, но идея «бери лучшее сейчас» — одна из самых интуитивных в алгоритмах.

## Как работают и сколько стоят

### Two Pointers

Ставишь два указателя — обычно в начало и конец массива. Двигаешь навстречу друг другу, сравнивая значения. Вместо того чтобы проверять каждый элемент с каждым (O(n²)), проходишь массив один раз.

- **Время:** O(n) — один проход
- **Память:** O(1) — ничего лишнего, только два указателя
- **Было бы без паттерна:** O(n²) — два вложенных цикла

### Sliding Window

Создаёшь «окно» — отрезок массива от left до right. Двигаешь правую границу вправо, добавляя элемент. Если условие нарушилось — сдвигаешь левую, убирая элемент. Окно всегда содержит текущий «кандидат» на ответ.

- **Время:** O(n) — каждый элемент входит в окно и выходит максимум один раз
- **Память:** O(1) или O(k) — зависит от того, что хранишь в окне
- **Было бы без паттерна:** O(n × k) — для каждой позиции пересчитывать весь отрезок

### Binary Search

Берёшь середину. Если цель больше — ищешь в правой половине. Меньше — в левой. Каждый шаг отбрасывает половину данных.

- **Время:** O(log n) — при n = 1 000 000 нужно всего ~20 шагов
- **Память:** O(1)
- **Было бы без паттерна:** O(n) — перебор от начала до конца

### HashMap / HashSet

Кладёшь элемент — он попадает в ячейку по хешу. Ищешь — вычисляешь хеш, идёшь сразу в нужную ячейку. Не перебираешь ничего.

- **Время:** O(1) на вставку и поиск (в среднем)
- **Память:** O(n) — хранишь все элементы
- **Было бы без паттерна:** O(n) на каждый поиск — проход по всему массиву

### Stack

Кладёшь элемент на вершину (push). Забираешь с вершины (pop). Заглядываешь на вершину (peek). Три операции — все за O(1). Задачи на стек обычно про обработку последовательности, где важен порядок: скобки, вложенность, ближайший больший элемент.

- **Время:** O(n) — один проход по данным
- **Память:** O(n) — в худшем случае весь массив ляжет в стек
- **Было бы без паттерна:** O(n²) — для каждого элемента искать пару вложенным циклом

### Trees / BFS / DFS

**BFS** — очередь. Кладёшь корень, достаёшь, кладёшь его детей, достаёшь первого ребёнка, кладёшь его детей... Обходишь уровень за уровнем.

**DFS** — стек (или рекурсия). Идёшь вглубь до упора, возвращаешься, пробуешь следующую ветку.

- **Время:** O(n) — посещаешь каждый узел один раз
- **Память:** O(n) для BFS (очередь может хранить целый уровень), O(h) для DFS (h — глубина дерева)
- **Когда что:** BFS — кратчайший путь, поуровневый обход. DFS — поиск пути, проверка свойств дерева

### Dynamic Programming

Разбиваешь задачу на подзадачи. Решаешь маленькие, запоминаешь ответы в массив. Большие задачи собираешь из готовых ответов. Два подхода: **снизу вверх** (таблица, цикл) и **сверху вниз** (рекурсия + кеш).

- **Время:** O(n) или O(n²) — зависит от задачи, но всегда лучше полного перебора
- **Память:** O(n) — таблица с ответами (иногда можно сжать до O(1))
- **Было бы без паттерна:** O(2ⁿ) — экспоненциальный перебор всех вариантов

### Greedy

На каждом шаге делаешь локально лучший выбор. Не откатываешься, не пересматриваешь. Работает только когда локальный оптимум гарантирует глобальный (это надо доказать или почувствовать).

- **Время:** O(n log n) — обычно нужна сортировка, потом один проход O(n)
- **Память:** O(1) — решения принимаются на лету
- **Было бы без паттерна:** O(2ⁿ) — перебор всех подмножеств

## Почему именно эти паттерны

Это не случайный набор. Каждый паттерн — ответ на фундаментальную проблему:

| Проблема | Паттерн |
|----------|---------|
| Перебор всех пар — медленно | Two Pointers |
| Нужен кусок массива, который двигается | Sliding Window |
| Поиск в отсортированных данных | Binary Search |
| «Видели ли мы это раньше?» | HashMap |
| Вложенность, отмена, порядок LIFO | Stack |
| Иерархия, связи между элементами | Trees |
| Повторяющиеся подзадачи | Dynamic Programming |
| Локально лучший выбор = глобально лучший | Greedy |

Всё остальное — комбинации и вариации.

## Структуры данных под капотом

Паттерны выше — это приёмы решения задач. Но они работают поверх структур данных. Некоторые из этих структур сложнее, чем кажутся снаружи. Вот что прячется внутри стандартных Java-классов.

### Красно-чёрное дерево (Red-Black Tree)

**Откуда:** придумал **Rudolf Bayer** в 1972 (назвал «symmetric binary B-tree»). В 1978 **Leonidas Guibas** и **Robert Sedgewick** перекрасили узлы в красный и чёрный — так появилось название.

**Что это:** бинарное дерево поиска, которое само себя балансирует. Обычное дерево может выродиться в «палку» — все элементы в одну сторону, и поиск становится O(n). Красно-чёрное дерево после каждой вставки/удаления подкручивает себя (вращения + перекраска), чтобы оставаться плоским.

**Правила простые:** корень чёрный, красный узел не может иметь красного ребёнка, любой путь от корня до листа содержит одинаковое количество чёрных узлов. Этого достаточно, чтобы дерево не перекосилось.

- **Время:** O(log n) на поиск, вставку, удаление — гарантированно
- **Где в Java:** `TreeMap`, `TreeSet`, внутри `HashMap` (когда в одном бакете >8 коллизий — связный список превращается в красно-чёрное дерево, с Java 8)

### AVL-дерево

**Откуда:** **Adelson-Velsky** и **Landis**, 1962, СССР. Первое в истории самобалансирующееся дерево. Названо по инициалам авторов.

**Что это:** похоже на красно-чёрное, но строже. Разница высот левого и правого поддерева — не больше 1. Из-за этого AVL ищет чуть быстрее (дерево более плоское), но вставка/удаление дороже (больше вращений).

- **Время:** O(log n) — как красно-чёрное, но поиск на практике быстрее, а вставка медленнее
- **Где в Java:** не используется в стандартной библиотеке. Java выбрала красно-чёрное дерево — оно проще и быстрее на вставках

### Куча (Heap)

**Откуда:** **J.W.J. Williams** в 1964 — придумал для алгоритма HeapSort.

**Что это:** почти полное бинарное дерево, где родитель всегда меньше (min-heap) или больше (max-heap) своих детей. Хранится в обычном массиве: дети элемента `i` лежат в `2i+1` и `2i+2`. Никаких указателей, никаких узлов — просто массив.

**Фишка:** достать минимум/максимум — O(1), просто берёшь первый элемент. Вставить или удалить — O(log n), элемент «всплывает» или «тонет» на своё место.

- **Время:** O(1) — peek, O(log n) — add/poll
- **Память:** O(n) — обычный массив
- **Где в Java:** `PriorityQueue` — это min-heap

### Связный список (LinkedList)

**Откуда:** **Allen Newell**, **Cliff Shaw**, **Herbert Simon**, 1955–1956, для программы Logic Theorist — одной из первых программ искусственного интеллекта.

**Что это:** цепочка узлов, каждый хранит значение и ссылку на следующий (и предыдущий в двусвязном). Вставить/удалить в середину — O(1), если есть ссылка на место. Но найти это место — O(n), потому что нельзя прыгнуть к элементу по индексу.

- **Время:** O(1) вставка/удаление по ссылке, O(n) поиск по индексу
- **Память:** больше массива — каждый узел хранит ссылки
- **Где в Java:** `LinkedList` (двусвязный список, реализует и List, и Deque)

### B-дерево (B-Tree)

**Откуда:** **Rudolf Bayer** и **Edward McCreight**, 1970, Boeing Research Labs. Создано для работы с диском — когда данные не влезают в оперативную память.

**Что это:** дерево, где каждый узел хранит не одно значение, а много (десятки-сотни). И детей у узла тоже много. За одно чтение с диска получаешь целый блок данных. Поэтому B-дерево — основа всех баз данных и файловых систем.

- **Время:** O(log n), но с огромным основанием логарифма — на практике 3-4 уровня хватает на миллиарды записей
- **Где:** PostgreSQL, MySQL, файловые системы (NTFS, ext4). В Java напрямую нет, но `TreeMap` — это его «младший брат»

### Итого: что когда использовать в Java

| Нужно | Класс Java | Внутри |
|-------|-----------|--------|
| Ключ-значение, максимальная скорость | `HashMap` | массив + связные списки + красно-чёрные деревья |
| Ключ-значение, отсортированные ключи | `TreeMap` | красно-чёрное дерево |
| Уникальные элементы, быстрый поиск | `HashSet` | `HashMap` внутри |
| Уникальные элементы, отсортированные | `TreeSet` | `TreeMap` внутри |
| Очередь с приоритетом, «дай минимум» | `PriorityQueue` | куча (min-heap) |
| Стек, очередь, дек | `ArrayDeque` | кольцевой массив |
| Частая вставка/удаление в середину | `LinkedList` | двусвязный список |
